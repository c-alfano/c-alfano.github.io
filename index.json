
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am an Applied Scientist at Amazon , working on training LLM-based evaluators.\nI completed my PhD in the Department of Statistics at the University of Oxford, under the supervision of Patrick Rebeschini and George Deligiannidis. I was funded by EPSRC.\nMy research interests include reinforcement learning, LLM fine-tuning, optimization and learning theory. In particular, I focus on building and analyzing reinforcement learning algorithms using standard optimization tools, such as natural gradient descent and mirror descent.\nDownload my CV. ","date":1774051200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1774051200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an Applied Scientist at Amazon , working on training LLM-based evaluators.\nI completed my PhD in the Department of Statistics at the University of Oxford, under the supervision of Patrick Rebeschini and George Deligiannidis.","tags":null,"title":"Carlo Alfano","type":"authors"},{"authors":["Carlo Alfano","Aymen Al Marjani","Zeno Jonke","Amin Mantrach","Saab Mansour","Marcello Federico"],"categories":null,"content":"","date":1774051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1774051200,"objectID":"367dac1b8c96e9b16db5e4069e38e9de","permalink":"https://c-alfano.github.io/publication/paper-6/","publishdate":"2026-03-21T00:00:00Z","relpermalink":"/publication/paper-6/","section":"publication","summary":"The growing use of large language models (LLMs) has increased the need for automatic evaluation systems, particularly to address the challenge of information hallucination. Although existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators that can operate across languages without extensive labeled data. This paper presents Self-Taught Evaluators for Multilingual Faithfulness, a framework that learns exclusively from synthetic multilingual summarization data while leveraging cross-lingual transfer learning. Through experiments comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent relationship between an LLM's general language capabilities and its performance in language-specific evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art English evaluators and machine translation-based approaches.","tags":null,"title":"Multilingual Self-Taught Faithfulness Evaluators","type":"publication"},{"authors":["Carlo Alfano","Silvia Sapora","Jakob Nicolaus Foerster","Patrick Rebeschini","Yee Whye Teh"],"categories":null,"content":"","date":1758585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1758585600,"objectID":"19f3b58fbbdcf9ec55f30f4b39b5ada7","permalink":"https://c-alfano.github.io/publication/paper-5/","publishdate":"2025-09-23T00:00:00Z","relpermalink":"/publication/paper-5/","section":"publication","summary":"Evaluating preference optimization (PO) algorithms on LLM alignment is a challenging task that presents prohibitive costs, noise, and several variables like model size and hyper-parameters. In this work, we show that it is possible to gain insights on the efficacy of PO algorithm on simpler benchmarks. We design a diagnostic suite of MuJoCo tasks and datasets, which we use to systematically evaluate PO algorithms, establishing a more controlled and cheaper benchmark. We then propose a novel family of PO algorithms based on mirror descent, which we call Mirror Preference Optimization (MPO). Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets, such as mixed-quality or noisy data. We demonstrate that our discovered PO algorithms outperform all known algorithms in the targeted MuJoCo settings. Finally, based on the insights gained from our MuJoCo experiments, we design a PO algorithm that significantly outperform existing baselines in an LLM alignment task.","tags":null,"title":"Meta-Learning Objectives for Preference Optimization","type":"publication"},{"authors":["Carlo Alfano","Sebastian Rene Towers","Silvia Sapora","Chris Lu","Patrick Rebeschini"],"categories":null,"content":"","date":1745452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745452800,"objectID":"7f3264d0b26ab39a388c31d812b0c9e5","permalink":"https://c-alfano.github.io/publication/paper-4/","publishdate":"2025-04-24T00:00:00Z","relpermalink":"/publication/paper-4/","section":"publication","summary":"Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map---namely, the negative entropy---which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e. Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments.","tags":null,"title":"Learning mirror maps in policy mirror descent","type":"publication"},{"authors":["Carlo Alfano","Rui Yuan","Patrick Rebeschini"],"categories":null,"content":"","date":1675123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675123200,"objectID":"bc31ce7548cb46636b50a17e79364d8b","permalink":"https://c-alfano.github.io/publication/paper-3/","publishdate":"2023-01-31T00:00:00Z","relpermalink":"/publication/paper-3/","section":"publication","summary":"Modern policy optimization methods in reinforcement learning, such as Trust Region Policy Optimization and Proximal Policy Optimization, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks and show that it represents an improvement upon the previous best results.","tags":null,"title":"A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence","type":"publication"},{"authors":["Carlo Alfano","Patrick Rebeschini"],"categories":null,"content":"","date":1664496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664496000,"objectID":"cc2d0d105c2d2dbd60328e36ba512da5","permalink":"https://c-alfano.github.io/publication/paper-2/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/publication/paper-2/","section":"publication","summary":"We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q-value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy class.","tags":null,"title":"Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization","type":"publication"},{"authors":["Carlo Alfano"],"categories":null,"content":"","date":1663768800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663768800,"objectID":"530bc10b2bf3d1747480fba95a3074b8","permalink":"https://c-alfano.github.io/talk/linear-convergence-for-natural-policy-gradient-with-log-linear-policy-parametrization/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/linear-convergence-for-natural-policy-gradient-with-log-linear-policy-parametrization/","section":"event","summary":"We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy.","tags":[],"title":"Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization","type":"event"},{"authors":["Carlo Alfano","Patrick Rebeschini"],"categories":null,"content":"","date":1632355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"9c180c49ce06dc4a6248c7cd268b05ed","permalink":"https://c-alfano.github.io/publication/paper-1/","publishdate":"2021-09-23T00:00:00Z","relpermalink":"/publication/paper-1/","section":"publication","summary":"Cooperative multi-agent reinforcement learning is a decentralized paradigm in sequential decision making where agents distributed over a network iteratively collaborate with neighbors to maximize global (network-wide) notions of rewards. Exact computations typically involve a complexity that scales exponentially with the number of agents. To address this curse of dimensionality, we design a scalable algorithm based on the Natural Policy Gradient framework that uses local information and only requires agents to communicate with neighbors within a certain range. Under standard assumptions on the spatial decay of correlations for the transition dynamics of the underlying Markov process and the localized learning policy, we show that our algorithm converges to the globally optimal policy with a dimension-free statistical and computational complexity, incurring a localization error that does not depend on the number of agents and converges to zero exponentially fast as a function of the range of communication.","tags":null,"title":"Dimension-Free Rates for Natural Policy Gradient in Multi-Agent Reinforcement Learning","type":"publication"}]