
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a final year PhD student in the Department of Statistics at the University of Oxford, under the supervision of Patrick Rebeschini and George Deligiannidis. I am funded by EPSRC.\nMy research interests include reinforcement learning, LLM fine-tuning, optimization and learning theory. In particular, I focus on building and analyzing reinforcement learning algorithms using standard optimization tools, such as natural gradient descent and mirror descent.\nDownload my CV. ","date":1758585600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1758585600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a final year PhD student in the Department of Statistics at the University of Oxford, under the supervision of Patrick Rebeschini and George Deligiannidis. I am funded by EPSRC.","tags":null,"title":"Carlo Alfano","type":"authors"},{"authors":["Carlo Alfano","Silvia Sapora","Jakob Nicolaus Foerster","Patrick Rebeschini","Yee Whye Teh"],"categories":null,"content":"","date":1758585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1758585600,"objectID":"19f3b58fbbdcf9ec55f30f4b39b5ada7","permalink":"https://c-alfano.github.io/publication/paper-5/","publishdate":"2025-09-23T00:00:00Z","relpermalink":"/publication/paper-5/","section":"publication","summary":"Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map---namely, the negative entropy---which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e. Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments.","tags":null,"title":"Meta-Learning Objectives for Preference Optimization","type":"publication"},{"authors":["Carlo Alfano","Sebastian Rene Towers","Silvia Sapora","Chris Lu","Patrick Rebeschini"],"categories":null,"content":"","date":1745452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745452800,"objectID":"7f3264d0b26ab39a388c31d812b0c9e5","permalink":"https://c-alfano.github.io/publication/paper-4/","publishdate":"2025-04-24T00:00:00Z","relpermalink":"/publication/paper-4/","section":"publication","summary":"Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map---namely, the negative entropy---which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e. Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments.","tags":null,"title":"Learning mirror maps in policy mirror descent","type":"publication"},{"authors":["Carlo Alfano","Rui Yuan","Patrick Rebeschini"],"categories":null,"content":"","date":1675123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675123200,"objectID":"bc31ce7548cb46636b50a17e79364d8b","permalink":"https://c-alfano.github.io/publication/paper-3/","publishdate":"2023-01-31T00:00:00Z","relpermalink":"/publication/paper-3/","section":"publication","summary":"Modern policy optimization methods in reinforcement learning, such as Trust Region Policy Optimization and Proximal Policy Optimization, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks and show that it represents an improvement upon the previous best results.","tags":null,"title":"A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence","type":"publication"},{"authors":["Carlo Alfano","Patrick Rebeschini"],"categories":null,"content":"","date":1664496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664496000,"objectID":"cc2d0d105c2d2dbd60328e36ba512da5","permalink":"https://c-alfano.github.io/publication/paper-2/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/publication/paper-2/","section":"publication","summary":"We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q-value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy class.","tags":null,"title":"Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization","type":"publication"},{"authors":["Carlo Alfano"],"categories":null,"content":"","date":1663768800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663768800,"objectID":"530bc10b2bf3d1747480fba95a3074b8","permalink":"https://c-alfano.github.io/talk/linear-convergence-for-natural-policy-gradient-with-log-linear-policy-parametrization/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/linear-convergence-for-natural-policy-gradient-with-log-linear-policy-parametrization/","section":"event","summary":"We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy.","tags":[],"title":"Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization","type":"event"},{"authors":["Carlo Alfano","Patrick Rebeschini"],"categories":null,"content":"","date":1632355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"9c180c49ce06dc4a6248c7cd268b05ed","permalink":"https://c-alfano.github.io/publication/paper-1/","publishdate":"2021-09-23T00:00:00Z","relpermalink":"/publication/paper-1/","section":"publication","summary":"Cooperative multi-agent reinforcement learning is a decentralized paradigm in sequential decision making where agents distributed over a network iteratively collaborate with neighbors to maximize global (network-wide) notions of rewards. Exact computations typically involve a complexity that scales exponentially with the number of agents. To address this curse of dimensionality, we design a scalable algorithm based on the Natural Policy Gradient framework that uses local information and only requires agents to communicate with neighbors within a certain range. Under standard assumptions on the spatial decay of correlations for the transition dynamics of the underlying Markov process and the localized learning policy, we show that our algorithm converges to the globally optimal policy with a dimension-free statistical and computational complexity, incurring a localization error that does not depend on the number of agents and converges to zero exponentially fast as a function of the range of communication.","tags":null,"title":"Dimension-Free Rates for Natural Policy Gradient in Multi-Agent Reinforcement Learning","type":"publication"}]