---
abstract: We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy. 
address:
  city: Oxford
  country: United Kingdom
  postcode: "OX2 6GG"
all_day: false
authors: [Carlo Alfano]
date: "2022-09-21T14:00:00Z"
event: 4th IMA Conference on The Mathematical Challenges of Big Data
event_url: https://ima.org.uk/17625/4th-ima-conference-on-the-mathematical-challenges-of-big-data/
featured: false
location: Oxford Mathematical Institute
projects:
- example
publishDate: "2017-01-01T00:00:00Z"
slides:
summary:
tags: []
title: Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization
url_code: ""
url_pdf: ""
url_slides: "https://raw.githubusercontent.com/c-alfano/c-alfano.github.io/main/uploads/Talks/Talk1.pdf"
url_video: ""
---
