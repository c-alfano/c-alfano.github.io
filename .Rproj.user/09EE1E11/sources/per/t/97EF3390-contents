---
abstract: Cooperative multi-agent reinforcement learning is a decentralized paradigm in sequential decision making where agents distributed over a network iteratively collaborate with neighbors to maximize global (network-wide) notions of rewards. Exact computations typically involve a complexity that scales exponentially with the number of agents. To address this curse of dimensionality, we design a scalable algorithm based on the Natural Policy Gradient framework that uses local information and only requires agents to communicate with neighbors within a certain range. Under standard assumptions on the spatial decay of correlations for the transition dynamics of the underlying Markov process and the localized learning policy, we show that our algorithm converges to the globally optimal policy with a dimension-free statistical and computational complexity, incurring a localization error that does not depend on the number of agents and converges to zero exponentially fast as a function of the range of communication.
authors:
- admin
- Patrick Rebeschini
date: "2021-09-23T00:00:00Z"
doi: ""
featured: false
#image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
#  focal_point: ""
#  preview_only: false
#links:
#- name: Custom Link
#  url: http://example.org
projects:
- internal-project
publication: "arXiv preprint: 2109.11692"
publication_short: ""
#publication_types:
#- "3"
publishDate: "2021-09-23"
slides:
summary:
#tags:
#- Source Themes
title: Dimension-Free Rates for Natural Policy Gradient in Multi-Agent Reinforcement Learning
#url_code: https://github.com/wowchemy/wowchemy-hugo-themes
#url_dataset: '#'
url_pdf: https://arxiv.org/abs/2109.11692
#url_poster: '#'
#url_project: ""
#url_slides: ""
#url_source: '#'
#url_video: '#'
---

