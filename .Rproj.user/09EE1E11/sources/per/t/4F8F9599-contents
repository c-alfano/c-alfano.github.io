---
abstract: Modern policy optimization methods in applied reinforcement learning are often inspired by the trust region policy optimization algorithm, which can be interpreted as a particular instance of policy mirror descent. While theoretical guarantees have been established for this framework, particularly in the tabular setting, the use of a general parametrization scheme remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parametrizations. The policy class induced by our scheme recovers known classes, e.g. tabular softmax, log-linear, and neural policies. It also generates new ones, depending on the choice of the mirror map. For a general mirror map and parametrization function, we establish the quasi-monotonicity of the updates in value function, global linear convergence rates, and we bound the total variation of the algorithm along its path. To showcase the ability of our framework to accommodate general parametrization schemes, we present a case study involving shallow neural networks.
authors:
- admin
- Rui Yuan
- Patrick Rebeschini
date: "2023-01-31T00:00:00Z"
doi: ""
featured: false
#image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
#  focal_point: ""
#  preview_only: false
#links:
#- name: Custom Link
#  url: http://example.org
projects:
- internal-project
publication: "arXiv preprint: 2301.13139. Submitted as a conference paper"
publication_short: ""
#publication_types:
#- "3"
publishDate: "2023-01-31"
slides:
summary:
#tags:
#- Source Themes
title: A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence
#url_code: https://github.com/wowchemy/wowchemy-hugo-themes
#url_dataset: '#'
url_pdf: https://arxiv.org/pdf/2301.13139.pdf
#url_poster: '#'
#url_project: ""
#url_slides: ""
#url_source: '#'
#url_video: '#'
---

